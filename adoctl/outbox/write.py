from __future__ import annotations

import datetime as dt
import html
import json
import re
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple

import yaml

from adoctl.ado_client.http import ado_patch_json_patch, ado_post_json_patch
from adoctl.ado_client.models import ADOConfig
from adoctl.config.contract_loader import EffectiveContractConfig, load_effective_contract
from adoctl.config.paths import outbox_dir
from adoctl.util.fs import atomic_write_text, ensure_dir
from adoctl.util.url import join_url
from adoctl.util.yaml_emit import render_yaml_with_header


CREATE_ORDER = {"Feature": 0, "UserStory": 1}
WORK_ITEM_REGISTRY_FILENAME = "_written_work_items.yaml"

CreateRequest = Callable[[ADOConfig, str, Sequence[Dict[str, Any]]], Dict[str, Any]]
LinkRequest = Callable[[ADOConfig, str, Sequence[Dict[str, Any]]], Dict[str, Any]]


def _now_utc() -> str:
    return dt.datetime.now(dt.timezone.utc).isoformat()


def _as_string(value: Any) -> Optional[str]:
    if not isinstance(value, str):
        return None
    cleaned = value.strip()
    return cleaned if cleaned else None


def _is_non_empty(value: Any) -> bool:
    if value is None:
        return False
    if isinstance(value, str):
        return bool(value.strip())
    if isinstance(value, (list, dict, tuple, set)):
        return len(value) > 0
    return True


def _is_under_directory(path: Path, parent: Path) -> bool:
    try:
        path.resolve().relative_to(parent.resolve())
        return True
    except ValueError:
        return False


def _unique_file_path(directory: Path, filename: str) -> Path:
    ensure_dir(directory)
    candidate = directory / filename
    if not candidate.exists():
        return candidate

    stem = candidate.stem
    suffix = candidate.suffix
    index = 1
    while True:
        numbered = directory / f"{stem}.{index}{suffix}"
        if not numbered.exists():
            return numbered
        index += 1


def _audit_filename(prefix: str) -> str:
    timestamp = dt.datetime.now(dt.timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    return f"{timestamp}_{prefix}.yaml"


def _registry_path(outbox_root: Path) -> Path:
    return outbox_root / WORK_ITEM_REGISTRY_FILENAME


def _redact_payload(value: Any) -> Any:
    if isinstance(value, dict):
        redacted: Dict[str, Any] = {}
        for key, nested in value.items():
            lowered = str(key).lower()
            if any(token in lowered for token in ("authorization", "pat", "token", "secret")):
                redacted[key] = "***REDACTED***"
            else:
                redacted[key] = _redact_payload(nested)
        return redacted
    if isinstance(value, list):
        return [_redact_payload(item) for item in value]
    return value


def _write_audit(audit_payload: Dict[str, Any], audit_path: Path) -> None:
    atomic_write_text(
        audit_path,
        render_yaml_with_header(
            _redact_payload(audit_payload),
            [
                "MACHINE-GENERATED FILE. DO NOT EDIT BY HAND.",
                "Generated by `adoctl write`.",
                "Contains write/dry-run plan, requests, response IDs, and fail-fast status.",
            ],
        ),
    )


def _load_work_item_registry(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {
            "schema_version": "1.0",
            "updated_at_utc": _now_utc(),
            "local_id_index": {},
        }

    with path.open("r", encoding="utf-8") as f:
        payload = yaml.safe_load(f) or {}
    if not isinstance(payload, dict):
        raise ValueError(f"Invalid registry payload in {path}.")

    raw_index = payload.get("local_id_index", {})
    if not isinstance(raw_index, dict):
        raise ValueError(f"Invalid local_id_index in registry {path}.")

    normalized_index: Dict[str, Dict[str, Any]] = {}
    for local_id, entry in raw_index.items():
        if not isinstance(local_id, str) or not local_id.strip() or not isinstance(entry, dict):
            continue
        raw_ado_id = entry.get("ado_id")
        ado_id: Optional[int] = None
        if isinstance(raw_ado_id, int):
            ado_id = raw_ado_id
        elif isinstance(raw_ado_id, str) and raw_ado_id.isdigit():
            ado_id = int(raw_ado_id)
        if ado_id is None:
            continue
        normalized_index[local_id.strip()] = {
            "ado_id": ado_id,
            "canonical_type": _as_string(entry.get("canonical_type")),
            "title": _as_string(entry.get("title")),
            "source_bundle_id": _as_string(entry.get("source_bundle_id")),
            "written_at_utc": _as_string(entry.get("written_at_utc")) or _now_utc(),
        }

    return {
        "schema_version": "1.0",
        "updated_at_utc": _as_string(payload.get("updated_at_utc")) or _now_utc(),
        "local_id_index": normalized_index,
    }


def _save_work_item_registry(registry_payload: Dict[str, Any], path: Path) -> None:
    local_id_index = registry_payload.get("local_id_index")
    if not isinstance(local_id_index, dict):
        raise ValueError("Registry payload missing local_id_index mapping.")
    payload = {
        "schema_version": "1.0",
        "updated_at_utc": _now_utc(),
        "local_id_index": {
            key: local_id_index[key]
            for key in sorted(local_id_index.keys())
            if isinstance(key, str) and key.strip()
        },
    }
    atomic_write_text(
        path,
        render_yaml_with_header(
            payload,
            [
                "MACHINE-GENERATED FILE. DO NOT EDIT BY HAND.",
                "Generated by `adoctl write`.",
                "Stores persistent local_id -> ado_id mappings for written work items.",
            ],
        ),
    )


def _lookup_registered_ado_id(registry_payload: Dict[str, Any], local_id: str) -> Optional[int]:
    local_id_index = registry_payload.get("local_id_index")
    if not isinstance(local_id_index, dict):
        return None
    entry = local_id_index.get(local_id)
    if not isinstance(entry, dict):
        return None
    raw_ado_id = entry.get("ado_id")
    if isinstance(raw_ado_id, int):
        return raw_ado_id
    if isinstance(raw_ado_id, str) and raw_ado_id.isdigit():
        return int(raw_ado_id)
    return None


def _register_written_item(
    registry_payload: Dict[str, Any],
    local_id: str,
    ado_id: int,
    canonical_type: Optional[str],
    title: Optional[str],
    source_bundle_id: Optional[str],
) -> None:
    local_id_index = registry_payload.setdefault("local_id_index", {})
    if not isinstance(local_id_index, dict):
        raise ValueError("Invalid registry local_id_index structure.")

    existing_ado_id = _lookup_registered_ado_id(registry_payload, local_id)
    if existing_ado_id is not None and existing_ado_id != ado_id:
        raise ValueError(
            f"Registry conflict for local_id '{local_id}': existing ado_id={existing_ado_id}, new ado_id={ado_id}."
        )

    local_id_index[local_id] = {
        "ado_id": ado_id,
        "canonical_type": canonical_type,
        "title": title,
        "source_bundle_id": source_bundle_id,
        "written_at_utc": _now_utc(),
    }


def _acceptance_criteria_to_text(value: Any) -> Optional[str]:
    if isinstance(value, str):
        cleaned = value.strip()
        return cleaned if cleaned else None
    if not isinstance(value, list):
        return None
    items = [item.strip() for item in value if isinstance(item, str) and item.strip()]
    if not items:
        return None
    return "\n".join(items)


def _looks_like_html(value: str) -> bool:
    return bool(re.search(r"<\s*[a-zA-Z][^>]*>", value))


def _markdown_inline_to_html(value: str) -> str:
    escaped = html.escape(value)
    escaped = re.sub(
        r"\[([^\]]+)\]\(([^)]+)\)",
        lambda match: (
            f'<a href="{html.escape(match.group(2), quote=True)}">'
            f"{html.escape(match.group(1))}</a>"
        ),
        escaped,
    )
    escaped = re.sub(r"\*\*(.+?)\*\*", r"<strong>\1</strong>", escaped)
    escaped = re.sub(r"\*(.+?)\*", r"<em>\1</em>", escaped)
    return escaped


def _markdown_to_html(value: str) -> str:
    if _looks_like_html(value):
        return value

    lines = value.splitlines()
    blocks: List[str] = []
    paragraph_lines: List[str] = []
    list_items: List[str] = []
    list_type: Optional[str] = None

    def flush_paragraph() -> None:
        nonlocal paragraph_lines
        if not paragraph_lines:
            return
        rendered = "<br/>".join(_markdown_inline_to_html(item) for item in paragraph_lines)
        blocks.append(f"<p>{rendered}</p>")
        paragraph_lines = []

    def flush_list() -> None:
        nonlocal list_items, list_type
        if not list_items or not list_type:
            return
        joined = "".join(f"<li>{_markdown_inline_to_html(item)}</li>" for item in list_items)
        blocks.append(f"<{list_type}>{joined}</{list_type}>")
        list_items = []
        list_type = None

    for raw_line in lines:
        line = raw_line.rstrip()
        if not line.strip():
            flush_paragraph()
            flush_list()
            continue

        heading_match = re.match(r"^\s*(#{1,6})\s+(.+)$", line)
        unordered_match = re.match(r"^\s*[-*+]\s+(.+)$", line)
        ordered_match = re.match(r"^\s*\d+\.\s+(.+)$", line)

        if heading_match:
            flush_paragraph()
            flush_list()
            level = len(heading_match.group(1))
            text = _markdown_inline_to_html(heading_match.group(2).strip())
            blocks.append(f"<h{level}>{text}</h{level}>")
            continue

        if unordered_match:
            flush_paragraph()
            if list_type not in {None, "ul"}:
                flush_list()
            list_type = "ul"
            list_items.append(unordered_match.group(1).strip())
            continue

        if ordered_match:
            flush_paragraph()
            if list_type not in {None, "ol"}:
                flush_list()
            list_type = "ol"
            list_items.append(ordered_match.group(1).strip())
            continue

        flush_list()
        paragraph_lines.append(line.strip())

    flush_paragraph()
    flush_list()
    return "\n".join(blocks) if blocks else "<p></p>"


def _collect_local_id_map(work_items: Sequence[Dict[str, Any]]) -> Tuple[Dict[str, Dict[str, Any]], Set[str]]:
    by_local_id: Dict[str, Dict[str, Any]] = {}
    duplicates: Set[str] = set()
    for item in work_items:
        local_id = _as_string(item.get("local_id"))
        if not local_id:
            continue
        if local_id in by_local_id:
            duplicates.add(local_id)
            continue
        by_local_id[local_id] = item
    return by_local_id, duplicates


def _resolve_parent_reference(work_item: Dict[str, Any]) -> Optional[str]:
    relations = work_item.get("relations")
    if not isinstance(relations, dict):
        return None
    return _as_string(relations.get("parent_local_id"))


def _canonical_field_values(
    work_item: Dict[str, Any],
    default_area_path: Optional[str],
    default_iteration_path: Optional[str],
) -> Dict[str, Any]:
    values: Dict[str, Any] = {}

    title = _as_string(work_item.get("title"))
    if title:
        values["title"] = title

    description = _as_string(work_item.get("description"))
    if description:
        values["description"] = description

    acceptance_criteria = _acceptance_criteria_to_text(work_item.get("acceptance_criteria"))
    if acceptance_criteria:
        values["acceptance_criteria"] = acceptance_criteria

    fields = work_item.get("fields")
    if isinstance(fields, dict):
        for canonical_key, raw_value in fields.items():
            if not isinstance(canonical_key, str) or not canonical_key.strip():
                continue
            canonical_key = canonical_key.strip()
            value: Any = raw_value
            if canonical_key == "acceptance_criteria":
                value = _acceptance_criteria_to_text(raw_value)
            elif isinstance(raw_value, str):
                value = raw_value.strip()
            if _is_non_empty(value):
                values[canonical_key] = value

    if "area_path" not in values and default_area_path:
        values["area_path"] = default_area_path
    if "iteration_path" not in values and default_iteration_path:
        values["iteration_path"] = default_iteration_path

    return values


def _required_missing_fields(
    required_keys: Sequence[str],
    canonical_values: Dict[str, Any],
) -> List[str]:
    missing: List[str] = []
    for field_key in sorted(set(required_keys)):
        if not _is_non_empty(canonical_values.get(field_key)):
            missing.append(field_key)
    return missing


def _can_write_canonical_field(
    contract: EffectiveContractConfig,
    canonical_type: str,
    canonical_key: str,
    wit_field_reference_names: Set[str],
) -> bool:
    mapping = contract.field_map.canonical_to_ado.get(canonical_key)
    if mapping is None:
        return False
    if mapping.applies_to and canonical_type not in mapping.applies_to:
        return False
    return mapping.reference_name in wit_field_reference_names


def _merge_acceptance_into_description(
    description: Optional[str],
    acceptance_criteria_text: str,
) -> str:
    items = [item.strip() for item in acceptance_criteria_text.splitlines() if item.strip()]
    section = "Acceptance Criteria:\n" + "\n".join(f"- {item}" for item in items)
    if not description:
        return section
    return f"{description}\n\n{section}"


def _load_planning_context(generated_dir: Path) -> Dict[str, Any]:
    planning_path = generated_dir / "planning_context.yaml"
    if not planning_path.exists():
        return {}
    with planning_path.open("r", encoding="utf-8") as f:
        payload = yaml.safe_load(f) or {}
    if not isinstance(payload, dict):
        return {}
    return payload


def _normalize_identity(value: Optional[str]) -> Optional[str]:
    if not isinstance(value, str):
        return None
    cleaned = value.strip().lower()
    return cleaned if cleaned else None


def _owner_identity_indexes(
    planning_context: Dict[str, Any],
    team_name: Optional[str],
) -> Tuple[Dict[str, str], Dict[str, str]]:
    display_index: Dict[str, str] = {}
    unique_index: Dict[str, str] = {}

    def add_identity(identity_payload: Any) -> None:
        if not isinstance(identity_payload, dict):
            return
        display_name = identity_payload.get("display_name")
        unique_name = identity_payload.get("unique_name")
        if isinstance(display_name, str) and display_name.strip():
            display_index[display_name.strip().lower()] = display_name.strip()
        if isinstance(unique_name, str) and unique_name.strip():
            unique_index[unique_name.strip().lower()] = unique_name.strip()

    for identity in planning_context.get("project_assignable_identities", []):
        add_identity(identity)

    teams = planning_context.get("teams", [])
    if isinstance(teams, list):
        for team in teams:
            if not isinstance(team, dict):
                continue
            team_value = _as_string(team.get("name"))
            if team_name and team_value and team_value.lower() != team_name.lower():
                continue
            for identity in team.get("assignable_identities", []):
                add_identity(identity)

    return display_index, unique_index


def _resolve_owner_identity(
    owner_value: str,
    owner_identity_format: str,
    planning_context: Dict[str, Any],
    team_name: Optional[str],
) -> str:
    normalized_owner = _normalize_identity(owner_value)
    if not normalized_owner:
        return owner_value
    display_index, unique_index = _owner_identity_indexes(planning_context, team_name=team_name)
    if not display_index and not unique_index:
        raise ValueError(
            "Owner validation requires assignable identities from planning context. "
            "Run `adoctl sync --planning-only` before `adoctl write`."
        )

    display_match = display_index.get(normalized_owner)
    unique_match = unique_index.get(normalized_owner)

    if owner_identity_format == "display_name":
        if display_match:
            return display_match
        if unique_match:
            raise ValueError(
                f"Owner '{owner_value}' matches unique_name '{unique_match}', "
                "but policy requires display_name."
            )
    elif owner_identity_format == "unique_name":
        if unique_match:
            return unique_match
        if display_match:
            raise ValueError(
                f"Owner '{owner_value}' matches display_name '{display_match}', "
                "but policy requires unique_name."
            )
    else:  # either
        if display_match:
            return display_match
        if unique_match:
            return unique_match

    raise ValueError(
        f"Owner '{owner_value}' is not assignable for the resolved team/project context. "
        "Use a value from planning_context assignable identities."
    )


def _extract_ado_id(response_payload: Dict[str, Any], operation_url: str) -> int:
    raw_id = response_payload.get("id")
    if isinstance(raw_id, int):
        return raw_id
    if isinstance(raw_id, str) and raw_id.isdigit():
        return int(raw_id)
    raise ValueError(f"ADO response missing integer id for operation {operation_url}.")


def _resolve_parent_ado_id(
    parent_reference: str,
    local_to_ado: Dict[str, int],
    registry_payload: Dict[str, Any],
) -> int:
    local_match = local_to_ado.get(parent_reference)
    if local_match is not None:
        return local_match
    registered_id = _lookup_registered_ado_id(registry_payload, parent_reference)
    if registered_id is not None:
        return registered_id
    if parent_reference.isdigit():
        return int(parent_reference)
    raise ValueError(
        f"Unable to resolve parent reference '{parent_reference}'. "
        "Use a local_id in the same bundle, an existing registry local_id, or a numeric ADO work item id."
    )


def _build_create_operation(
    cfg: ADOConfig,
    contract: EffectiveContractConfig,
    work_item: Dict[str, Any],
    default_area_path: Optional[str],
    default_iteration_path: Optional[str],
    planning_context: Dict[str, Any],
    context_team_name: Optional[str],
    owner_override: Optional[str],
) -> Dict[str, Any]:
    local_id = _as_string(work_item.get("local_id"))
    if not local_id:
        raise ValueError("Work item missing local_id.")

    canonical_type = _as_string(work_item.get("type"))
    if not canonical_type:
        raise ValueError(f"Work item '{local_id}' missing type.")

    if canonical_type not in contract.wit_map.canonical_to_ado:
        raise ValueError(f"Unknown canonical type '{canonical_type}' for work item '{local_id}'.")

    ado_wit = contract.resolve_ado_wit(canonical_type)
    wit_contract = contract.generated_wit_contract.work_item_types.get(ado_wit)
    if wit_contract is None:
        raise ValueError(
            f"Generated metadata missing ADO WIT '{ado_wit}' for canonical type '{canonical_type}'."
        )

    canonical_values = _canonical_field_values(
        work_item=work_item,
        default_area_path=default_area_path,
        default_iteration_path=default_iteration_path,
    )
    warnings: List[str] = []

    if _is_non_empty(owner_override) and _can_write_canonical_field(
        contract=contract,
        canonical_type=canonical_type,
        canonical_key="owner",
        wit_field_reference_names=wit_contract.field_reference_names,
    ):
        canonical_values["owner"] = owner_override

    acceptance_criteria_text = _as_string(canonical_values.get("acceptance_criteria"))
    if acceptance_criteria_text and not _can_write_canonical_field(
        contract=contract,
        canonical_type=canonical_type,
        canonical_key="acceptance_criteria",
        wit_field_reference_names=wit_contract.field_reference_names,
    ):
        if not _can_write_canonical_field(
            contract=contract,
            canonical_type=canonical_type,
            canonical_key="description",
            wit_field_reference_names=wit_contract.field_reference_names,
        ):
            raise ValueError(
                f"Work item '{local_id}' includes acceptance_criteria, but ADO WIT '{ado_wit}' "
                "does not expose acceptance criteria or description fallback fields."
            )
        description_text = _as_string(canonical_values.get("description"))
        canonical_values["description"] = _merge_acceptance_into_description(
            description=description_text,
            acceptance_criteria_text=acceptance_criteria_text,
        )
        canonical_values.pop("acceptance_criteria", None)

    owner_value = _as_string(canonical_values.get("owner"))
    if owner_value:
        try:
            canonical_values["owner"] = _resolve_owner_identity(
                owner_value=owner_value,
                owner_identity_format=contract.field_policy.owner_identity_format,
                planning_context=planning_context,
                team_name=context_team_name,
            )
        except ValueError as exc:
            canonical_values.pop("owner", None)
            warnings.append(
                f"Owner '{owner_value}' is not assignable for this context; Assigned To will be null. Detail: {exc}"
            )

    description_value = _as_string(canonical_values.get("description"))
    if description_value and _can_write_canonical_field(
        contract=contract,
        canonical_type=canonical_type,
        canonical_key="description",
        wit_field_reference_names=wit_contract.field_reference_names,
    ):
        canonical_values["description"] = _markdown_to_html(description_value)

    required_keys = tuple(contract.effective_required_fields_by_type().get(canonical_type, set()))
    missing_fields = _required_missing_fields(required_keys, canonical_values)
    if missing_fields:
        raise ValueError(
            f"Work item '{local_id}' ({canonical_type}) missing required canonical fields: {missing_fields}."
        )

    patch_document: List[Dict[str, Any]] = []
    for canonical_key in sorted(canonical_values.keys()):
        mapping = contract.field_map.canonical_to_ado.get(canonical_key)
        if mapping is None:
            raise ValueError(
                f"Work item '{local_id}' uses unknown canonical field '{canonical_key}'. "
                "Add it to config/policy/field_map.yaml first."
            )
        if mapping.applies_to and canonical_type not in mapping.applies_to:
            raise ValueError(
                f"Canonical field '{canonical_key}' is not applicable to type '{canonical_type}' "
                f"for work item '{local_id}'."
            )
        if mapping.reference_name not in wit_contract.field_reference_names:
            raise ValueError(
                f"ADO field '{mapping.reference_name}' for canonical field '{canonical_key}' is unavailable on "
                f"ADO WIT '{ado_wit}'."
            )
        patch_document.append(
            {
                "op": "add",
                "path": f"/fields/{mapping.reference_name}",
                "value": canonical_values[canonical_key],
            }
        )

    if not patch_document:
        raise ValueError(f"Work item '{local_id}' resolved to an empty create payload.")

    create_url = join_url(cfg.org_url, cfg.project, "_apis", "wit", "workitems", f"${ado_wit}")
    return {
        "phase": "create",
        "local_id": local_id,
        "canonical_type": canonical_type,
        "method": "POST",
        "url": create_url,
        "body": patch_document,
        "warnings": warnings,
    }


def _build_link_operation(
    cfg: ADOConfig,
    local_id: str,
    child_ado_id: int,
    parent_ado_id: int,
) -> Dict[str, Any]:
    link_url = join_url(cfg.org_url, cfg.project, "_apis", "wit", "workitems", str(child_ado_id))
    parent_url = join_url(cfg.org_url, cfg.project, "_apis", "wit", "workitems", str(parent_ado_id))
    patch_document = [
        {
            "op": "add",
            "path": "/relations/-",
            "value": {
                "rel": "System.LinkTypes.Hierarchy-Reverse",
                "url": parent_url,
            },
        }
    ]
    return {
        "phase": "link",
        "local_id": local_id,
        "canonical_type": None,
        "method": "PATCH",
        "url": link_url,
        "body": patch_document,
        "parent_ado_id": parent_ado_id,
        "child_ado_id": child_ado_id,
    }


def _ordered_work_items(work_items: Sequence[Dict[str, Any]]) -> List[Dict[str, Any]]:
    def sort_key(item: Dict[str, Any]) -> Tuple[int, str]:
        canonical_type = _as_string(item.get("type")) or ""
        local_id = _as_string(item.get("local_id")) or ""
        order = CREATE_ORDER.get(canonical_type, 99)
        return (order, local_id)

    return sorted(work_items, key=sort_key)


def _load_bundle_payload(bundle_path: Path) -> Dict[str, Any]:
    payload = json.loads(bundle_path.read_text(encoding="utf-8"))
    if not isinstance(payload, dict):
        raise ValueError(f"Bundle JSON must decode to an object: {bundle_path}")
    return payload


def _bundle_work_items(bundle_payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    work_items_raw = bundle_payload.get("work_items")
    if not isinstance(work_items_raw, list):
        raise ValueError("Bundle work_items must be an array.")
    work_items = [item for item in work_items_raw if isinstance(item, dict)]
    if not work_items:
        raise ValueError("Bundle contains no valid work_items objects.")
    by_local_id, duplicates = _collect_local_id_map(work_items)
    if duplicates:
        raise ValueError(f"Bundle contains duplicate local_id values: {sorted(duplicates)}")
    if len(by_local_id) != len(work_items):
        raise ValueError("Bundle contains work items with missing local_id values.")
    return work_items


def _execute_operation(
    cfg: ADOConfig,
    operation: Dict[str, Any],
    create_request: CreateRequest,
    link_request: LinkRequest,
) -> Dict[str, Any]:
    method = operation["method"]
    url = operation["url"]
    body = operation["body"]
    if method == "POST":
        return create_request(cfg, url, body)
    if method == "PATCH":
        return link_request(cfg, url, body)
    raise ValueError(f"Unsupported method '{method}' in write operation.")


def _process_bundle(
    bundle_payload: Dict[str, Any],
    contract: EffectiveContractConfig,
    cfg: ADOConfig,
    dry_run: bool,
    default_area_path: Optional[str],
    default_iteration_path: Optional[str],
    planning_context: Dict[str, Any],
    registry_payload: Dict[str, Any],
    registry_file_path: Path,
    create_request: CreateRequest,
    link_request: LinkRequest,
    owner_override: Optional[str],
) -> Dict[str, Any]:
    try:
        work_items = _ordered_work_items(_bundle_work_items(bundle_payload))
    except Exception as exc:  # noqa: BLE001 - fail-fast writer behavior
        return {
            "result": "failed",
            "error": str(exc),
            "operations": [],
            "local_id_to_ado_id": {},
            "bundle_id": bundle_payload.get("bundle_id"),
        }

    local_to_ado: Dict[str, int] = {}
    operations: List[Dict[str, Any]] = []
    simulated_id = 100000
    context = bundle_payload.get("context")
    context_mapping = context if isinstance(context, dict) else {}
    context_team_name = _as_string(context_mapping.get("team"))

    for work_item in work_items:
        local_id = _as_string(work_item.get("local_id")) or "<unknown>"
        canonical_type = _as_string(work_item.get("type"))
        existing_ado_id = _lookup_registered_ado_id(registry_payload, local_id)
        if existing_ado_id is not None:
            local_to_ado[local_id] = existing_ado_id
            operations.append(
                {
                    "phase": "create",
                    "local_id": local_id,
                    "canonical_type": canonical_type,
                    "method": "SKIP",
                    "url": f"<already-written ado_id={existing_ado_id}>",
                    "request_body": [],
                    "executed_at_utc": _now_utc(),
                    "status": "existing",
                    "response": {"id": existing_ado_id},
                }
            )
            continue

        try:
            create_operation = _build_create_operation(
                cfg=cfg,
                contract=contract,
                work_item=work_item,
                default_area_path=default_area_path,
                default_iteration_path=default_iteration_path,
                planning_context=planning_context,
                context_team_name=context_team_name,
                owner_override=owner_override,
            )
        except Exception as exc:  # noqa: BLE001 - fail-fast writer behavior
            operations.append(
                {
                    "phase": "create",
                    "local_id": local_id,
                    "canonical_type": canonical_type,
                    "method": "POST",
                    "url": "<unresolved>",
                    "request_body": [],
                    "executed_at_utc": _now_utc(),
                    "status": "failed",
                    "error": str(exc),
                }
            )
            return {
                "result": "failed",
                "error": str(exc),
                "operations": operations,
                "local_id_to_ado_id": local_to_ado,
                "bundle_id": bundle_payload.get("bundle_id"),
            }

        create_record = {
            "phase": create_operation["phase"],
            "local_id": create_operation["local_id"],
            "canonical_type": create_operation["canonical_type"],
            "method": create_operation["method"],
            "url": create_operation["url"],
            "request_body": create_operation["body"],
            "executed_at_utc": _now_utc(),
            "warnings": list(create_operation.get("warnings", [])),
        }

        if dry_run:
            local_to_ado[create_operation["local_id"]] = simulated_id
            create_record["status"] = "simulated"
            create_record["response"] = {"id": simulated_id}
            simulated_id += 1
            operations.append(create_record)
        else:
            try:
                create_response = _execute_operation(
                    cfg=cfg,
                    operation=create_operation,
                    create_request=create_request,
                    link_request=link_request,
                )
                created_ado_id = _extract_ado_id(create_response, create_operation["url"])
                local_to_ado[create_operation["local_id"]] = created_ado_id
                _register_written_item(
                    registry_payload=registry_payload,
                    local_id=create_operation["local_id"],
                    ado_id=created_ado_id,
                    canonical_type=create_operation["canonical_type"],
                    title=_as_string(work_item.get("title")),
                    source_bundle_id=_as_string(bundle_payload.get("bundle_id")),
                )
                _save_work_item_registry(registry_payload, registry_file_path)
                create_record["status"] = "executed"
                create_record["response"] = {"id": created_ado_id}
                operations.append(create_record)
            except Exception as exc:  # noqa: BLE001 - fail-fast writer behavior
                create_record["status"] = "failed"
                create_record["error"] = str(exc)
                operations.append(create_record)
                return {
                    "result": "failed",
                    "error": str(exc),
                    "operations": operations,
                    "local_id_to_ado_id": local_to_ado,
                    "bundle_id": bundle_payload.get("bundle_id"),
                }

        parent_reference = _resolve_parent_reference(work_item)
        if not parent_reference:
            continue

        child_local_id = create_operation["local_id"]
        child_ado_id = local_to_ado[child_local_id]
        try:
            parent_ado_id = _resolve_parent_ado_id(
                parent_reference=parent_reference,
                local_to_ado=local_to_ado,
                registry_payload=registry_payload,
            )
            link_operation = _build_link_operation(
                cfg=cfg,
                local_id=child_local_id,
                child_ado_id=child_ado_id,
                parent_ado_id=parent_ado_id,
            )
        except Exception as exc:  # noqa: BLE001 - fail-fast writer behavior
            operations.append(
                {
                    "phase": "link",
                    "local_id": child_local_id,
                    "canonical_type": _as_string(work_item.get("type")),
                    "method": "PATCH",
                    "url": "<unresolved>",
                    "request_body": [],
                    "status": "failed",
                    "executed_at_utc": _now_utc(),
                    "error": str(exc),
                }
            )
            return {
                "result": "failed",
                "error": str(exc),
                "operations": operations,
                "local_id_to_ado_id": local_to_ado,
                "bundle_id": bundle_payload.get("bundle_id"),
            }

        link_record = {
            "phase": link_operation["phase"],
            "local_id": link_operation["local_id"],
            "canonical_type": _as_string(work_item.get("type")),
            "method": link_operation["method"],
            "url": link_operation["url"],
            "request_body": link_operation["body"],
            "executed_at_utc": _now_utc(),
        }

        if dry_run:
            link_record["status"] = "simulated"
            link_record["response"] = {"id": child_ado_id}
            operations.append(link_record)
            continue

        try:
            link_response = _execute_operation(
                cfg=cfg,
                operation=link_operation,
                create_request=create_request,
                link_request=link_request,
            )
            link_record["status"] = "executed"
            link_record["response"] = {"id": _extract_ado_id(link_response, link_operation["url"])}
            operations.append(link_record)
        except Exception as exc:  # noqa: BLE001 - fail-fast writer behavior
            link_record["status"] = "failed"
            link_record["error"] = str(exc)
            operations.append(link_record)
            return {
                "result": "failed",
                "error": str(exc),
                "operations": operations,
                "local_id_to_ado_id": local_to_ado,
                "bundle_id": bundle_payload.get("bundle_id"),
            }

    return {
        "result": "passed",
        "error": None,
        "operations": operations,
        "local_id_to_ado_id": local_to_ado,
        "bundle_id": bundle_payload.get("bundle_id"),
    }


def write_outbox(
    bundle: Optional[str],
    write_all_validated: bool,
    dry_run: bool,
    org_url: str,
    project: str,
    pat: Optional[str],
    api_version: str = "6.0",
    area_override: Optional[str] = None,
    iteration_override: Optional[str] = None,
    owner_display_name: Optional[str] = None,
    policy_dir: Optional[Path] = None,
    generated_dir: Optional[Path] = None,
    outbox_root: Optional[Path] = None,
    audit_root: Optional[Path] = None,
    create_request: Optional[CreateRequest] = None,
    link_request: Optional[LinkRequest] = None,
) -> Dict[str, Any]:
    if write_all_validated and bundle:
        raise ValueError("Pass either a bundle path or --all-validated, not both.")
    if not write_all_validated and not bundle:
        raise ValueError("Provide a bundle path or pass --all-validated.")
    if not org_url.strip():
        raise ValueError("org_url is required for write operations.")
    if not project.strip():
        raise ValueError("project is required for write operations.")
    if not dry_run and not _as_string(pat):
        raise ValueError("PAT is required for real writes. Use --pat-env to provide one.")

    resolved_outbox = outbox_root or outbox_dir()
    validated_dir = resolved_outbox / "validated"
    archived_dir = resolved_outbox / "archived"
    ensure_dir(validated_dir)
    ensure_dir(archived_dir)
    registry_file_path = _registry_path(resolved_outbox)
    registry_payload = _load_work_item_registry(registry_file_path)

    if write_all_validated:
        bundle_paths = sorted([path for path in validated_dir.glob("*.json") if path.is_file()], key=lambda p: p.name)
    else:
        bundle_path = Path(bundle).resolve() if bundle else None
        if bundle_path is None or not bundle_path.exists() or not bundle_path.is_file():
            raise FileNotFoundError(f"Bundle file not found: {bundle}")
        bundle_paths = [bundle_path]

    resolved_generated_dir = generated_dir if generated_dir is not None else Path("config") / "generated"
    contract = load_effective_contract(policy_dir=policy_dir, generated_dir=resolved_generated_dir)
    planning_context = _load_planning_context(resolved_generated_dir)
    cfg = ADOConfig(
        org_url=org_url.strip(),
        project=project.strip(),
        pat=pat or "",
        api_version=api_version,
    )

    create_call = create_request or ado_post_json_patch
    link_call = link_request or ado_patch_json_patch

    run_started = _now_utc()
    results: List[Dict[str, Any]] = []
    succeeded_count = 0
    failed_count = 0
    stopped_on_error = False

    for bundle_path in bundle_paths:
        managed_by_outbox = _is_under_directory(bundle_path, validated_dir)
        try:
            bundle_payload = _load_bundle_payload(bundle_path)
            context = bundle_payload.get("context")
            context_mapping = context if isinstance(context, dict) else {}

            resolved_area = _as_string(area_override) or _as_string(context_mapping.get("default_area_path"))
            resolved_iteration = _as_string(iteration_override) or _as_string(context_mapping.get("default_iteration_path"))

            result = _process_bundle(
                bundle_payload=bundle_payload,
                contract=contract,
                cfg=cfg,
                dry_run=dry_run,
                default_area_path=resolved_area,
                default_iteration_path=resolved_iteration,
                planning_context=planning_context,
                registry_payload=registry_payload,
                registry_file_path=registry_file_path,
                create_request=create_call,
                link_request=link_call,
                owner_override=_as_string(owner_display_name),
            )
        except Exception as exc:  # noqa: BLE001 - audit should still emit for unexpected failures
            result = {
                "result": "failed",
                "error": str(exc),
                "operations": [],
                "local_id_to_ado_id": {},
                "bundle_id": None,
            }

        moved_bundle_path: Optional[Path] = None
        if result["result"] == "passed":
            succeeded_count += 1
            if managed_by_outbox and not dry_run:
                destination = _unique_file_path(archived_dir, bundle_path.name)
                bundle_path.replace(destination)
                moved_bundle_path = destination
        else:
            failed_count += 1
            stopped_on_error = True

        results.append(
            {
                "bundle_path": str(bundle_path),
                "bundle_id": result.get("bundle_id"),
                "result": result["result"],
                "error": result.get("error"),
                "operations": result["operations"],
                "local_id_to_ado_id": result["local_id_to_ado_id"],
                "managed_by_outbox": managed_by_outbox,
                "moved_bundle_path": str(moved_bundle_path) if moved_bundle_path else None,
            }
        )

        if stopped_on_error:
            break

    run_finished = _now_utc()
    audit_payload = {
        "schema_version": "1.0",
        "run_started_at_utc": run_started,
        "run_finished_at_utc": run_finished,
        "mode": "dry-run" if dry_run else "write",
        "org_url": cfg.org_url,
        "project": cfg.project,
        "api_version": cfg.api_version,
        "summary": {
            "processed_count": len(results),
            "succeeded_count": succeeded_count,
            "failed_count": failed_count,
            "stopped_on_error": stopped_on_error,
        },
        "registry_path": str(registry_file_path),
        "inputs": {
            "bundle": bundle,
            "all_validated": write_all_validated,
            "area_override": _as_string(area_override),
            "iteration_override": _as_string(iteration_override),
            "owner_display_name": _as_string(owner_display_name),
        },
        "bundles": results,
    }

    resolved_audit_root = audit_root or (Path.cwd() / "audit")
    ensure_dir(resolved_audit_root)
    audit_path = _unique_file_path(resolved_audit_root, _audit_filename("write_audit"))
    _write_audit(audit_payload=audit_payload, audit_path=audit_path)

    return {
        "processed_count": len(results),
        "succeeded_count": succeeded_count,
        "failed_count": failed_count,
        "stopped_on_error": stopped_on_error,
        "dry_run": dry_run,
        "results": results,
        "audit_path": str(audit_path),
        "strict_ready": failed_count == 0,
    }
